#'===================================================================================================================#
#'
#' This script takes annual risk of infection (ARI) values developed by Pete Dodd and Rein Houben and 
#' generates force of infection (FOI) estimates for differing population groups (by age, year of arrival 
#' and country of birth), which can then be converted to probabilities/risk of infection and combined 
#' with census datasets to arrive at prevalence of TB infection estimates for a particular setting.
#' The script will  calculate the median hazard for each population group in the census and, additionally, will provide lower and 
#' upper percentile estimates, as defined by the following
#' objects.
#' These are also combined with TB data to provide estimates of reactivation rates/risks.
#'
#' INPUTS:
#'  - ARTI estimates generated by Pete Dodd and Rein Houben ("5000repLARI.Rdata" and "200repLARI.Rdata")
#'  - Census data by age, year of arrival, and country of birth. This file needs to be cleaned
#'  and reshaped, if necessary, with the "CleanseCensus" function to create a dataset with the
#'  following columns:
#'      -  NUMP - Number of persons in the population group
#'      -  AGEP - Age of population group
#'      -  YARP - Year of arrival of population group
#'      -  YOBP - Year of birth of population group
#'      -  BPLP - Birth country of population group
#'      -  ISO3 - 3 letter country code corresponding to birth country 
#'      -  CNSY - Census year
#'  - TB data by year of notification, age, year of arrival, and country of birth. This file needs to be cleaned
#'  and reshaped, if necessary, with the "CleanseTBdata" function to create a dataset with the
#'  following columns:
#'      -  year - Year of notification
#'      -  poptb - Number of TB cases in the population group
#'      -  AGEP - Age of population group
#'      -  YARP - Year of arrival of population group
#'      -  ISO3 - 3 letter country code corresponding to birth country 
#' 
#' OUTPUTS:
#' - A master table of all TB infection probabilities and outcomes, 
#' stored in the outputs path specified below (path.out).
#'      -  NUMP - Number of persons in the population group
#'      -  AGEP - Age of population group
#'      -  YARP - Year of arrival of population group
#'      -  YOBP - Year of birth of population group
#'      -  BPLP - Birth country of population group
#'      -  ISO3 - 3 letter country code corresponding to birth country 
#'      -  CNSY - Census year
#'      -  LTBP - Number of in the population estimated to have been infected
#'      -  PROB - Probability/risk of infection in the population group
#' 
#' Author: Katie Dale and Milinda Abayawardana
#' Date created: 2016-2021
#'===================================================================================================================#

rm(list = ls(all.names = TRUE)) # Clears all objects includes hidden objects.
gc() # Frees up memory and report the memory usage.

#' LOAD REQUIRED LIBRARIES ==========================================================================================#

library(data.table)
library(tidyverse)
library(reshape2)
library(countrycode)
library(openxlsx)

#' DEFINE USEFUL OBJECTS (INPUT & OUTPUT PATHS, FUNCTIONS, STUDY PERIOD ETC) ========================================#

#' The iso3 value for the country from which the census data is from

census.iso3 <- "CAN"

census.year <- 2021

#' This script will calculate the median hazard for each population
#' group in the census and, additionally, will provide lower and 
#' upper percentile estimates, as defined by the following
#' objects.
low.percentile <- 0.025
high.percentile <- 0.975

#' Data inputs file path
path.in <- "C:/Users/miche/Downloads/ARI project/"

#' Output file path
path.out <- "C:/Users/miche/Downloads/ARI project/"

#' Source all functions, which are located within the "Functions" file.
source(paste0(path.in, "Functions3 iso3.R"))

#' INPUTS ===========================================================================================================#

#' Load the hazard data from Houben & Dodd
# filename <- paste0(path.in, "5000repLARI.Rdata")
# 
# tbhaz.5000rep <- load(filename)
# tbhaz.5000rep <- LARI
# rm(LARI)

filename <- paste0(path.in, "200repLARI.Rdata") #lari upto 2014
load(filename)

filename2 <- paste0(path.in, "fcst.Rdata") #lari upto 2021
load(filename2)


tbhaz.200rep.bind <- rbind(rundata, fcst)
tbhaz.200rep <- as.data.table(tbhaz.200rep.bind)
rm(rundata)
View(tbhaz.200rep)
table(tbhaz.200rep.bind$iso3)

#making a new dataset with values upto 2050 for analysis
tbhaz.200rep.bind <- tbhaz.200rep.bind %>%
  mutate(ari = exp(lari)) %>%
  select(-lari)

#tbhaz_ari <- tbhaz.200rep.bind

# Calculate base ARIs for 2000 and 2021 for each country
base_ari <- tbhaz.200rep.bind %>%
  filter(year %in% c(2000, 2021)) %>%
  group_by(iso3, year) %>%
  summarize(ari = mean((ari), na.rm = TRUE)) %>%
  spread(key = year, value = ari)

colnames(base_ari)<-c("iso3","year_2000","year_2021")

# Calculate the annual rate of change and adjust for a 1-5% decrease(absolute change)
base_ari <- base_ari %>%
mutate(rate_of_change = (year_2021 / year_2000)^(1/22) - 1,
adjusted_rate_1 = rate_of_change - 0.01,
adjusted_rate_2 = rate_of_change - 0.02,
adjusted_rate_3 = rate_of_change - 0.03,
adjusted_rate_4 = rate_of_change - 0.04,
adjusted_rate_5 = rate_of_change - 0.05)
saveRDS(base_ari, file = paste0(path.out, "base.ari.rds"))


base_ari <- base_ari %>%
  mutate(
    status_quo_rate = rate_of_change + 1,
    adjusted_rate_1 = adjusted_rate_1 + 1,
    adjusted_rate_2 = adjusted_rate_2 + 1,
    adjusted_rate_3 = adjusted_rate_3 + 1,
    adjusted_rate_4 = adjusted_rate_4 + 1,
    adjusted_rate_5 = adjusted_rate_5 + 1
  )

# Prepare the data for ARI calculations
year2021_df <- tbhaz.200rep.bind[tbhaz.200rep.bind$year == 2021, c("year", "iso3", "replicate", "ari")]
year2021_df <- year2021_df[rep(seq_len(nrow(year2021_df)), each = 29), ]
year2021_df <- merge(year2021_df, base_ari, by = "iso3", all.x = TRUE)
year2021_df <- year2021_df %>%
  select(-c(year_2000, year_2021))

year2021_df$row_number <- ave(rep(1, nrow(year2021_df)),
                              year2021_df$iso3,
                              year2021_df$replicate,
                              FUN = seq_along)
year2021_df$year <- 2021 + year2021_df$row_number

# Apply ARI calculations for each year with reductions starting 2025
year2021_df <- year2021_df %>%
  mutate(
    # ARI calculations based on status quo rate (constant across all years)
    ari_sq = mapply(function(ari, rate, n) ari * (rate)^n, ari, status_quo_rate, row_number),
    
    # Explicitly match ari_sq for years < 2025 to avoid recalculations
    ari_1 = ifelse(year < 2025, ari_sq, 
                   mapply(function(ari, rate, n) ari * (rate)^n, ari, adjusted_rate_1, row_number)),
    ari_2 = ifelse(year < 2025, ari_sq, 
                   mapply(function(ari, rate, n) ari * (rate)^n, ari, adjusted_rate_2, row_number)),
    ari_3 = ifelse(year < 2025, ari_sq, 
                   mapply(function(ari, rate, n) ari * (rate)^n, ari, adjusted_rate_3, row_number)),
    ari_4 = ifelse(year < 2025, ari_sq, 
                   mapply(function(ari, rate, n) ari * (rate)^n, ari, adjusted_rate_4, row_number)),
    ari_5 = ifelse(year < 2025, ari_sq, 
                   mapply(function(ari, rate, n) ari * (rate)^n, ari, adjusted_rate_5, row_number))
  ) %>%
  # Remove unnecessary columns to clean up the dataset
  select(-c(row_number, ari, rate_of_change, adjusted_rate_1, adjusted_rate_2, adjusted_rate_3,
            adjusted_rate_4, adjusted_rate_5, status_quo_rate))

#Rename new ari columns for all % deccrease (sq indicates status quo- original)
tbhaz.200rep.bind <- tbhaz.200rep.bind %>%
  mutate(ari_sq = ari,
         ari_1 = ari,
         ari_2 = ari,
         ari_3 = ari,
         ari_4 = ari,
         ari_5 = ari) %>%
  select(- ari)

# Merge the 2 ari datasets
tbhaz.200rep.bind <- rbind(tbhaz.200rep.bind, year2021_df)
tbhaz.200rep <- tbhaz.200rep.bind 

saveRDS(tbhaz.200rep, file = paste0(path.out, "ari(all_countries2050).rds"))

#make log scale
tbhaz.200rep <- tbhaz.200rep %>%
  mutate(lari_sq = log(ari_sq),
         lari_1 = log(ari_1),
         lari_2 = log(ari_2),
         lari_3 = log(ari_3),
         lari_4 = log(ari_4),
         lari_5 = log(ari_5))

tbhaz.200rep <- tbhaz.200rep %>%
  select(-c (ari_sq, ari_1, ari_2, ari_3, ari_4, ari_5))

#Only use one lari column at once(functions cannot handle mutiple lari columns)
tbhaz.200repsq <- tbhaz.200rep %>% #Replace sq in 'tbhaz.200repsq' with any other decrease values as required
 mutate (lari = lari_sq) %>% #replace lari_sq with any other lari column as required
 select (- c(lari_sq, lari_1, lari_2, lari_3, lari_4, lari_5))
tbhaz.200reps5 <- as.data.table(tbhaz.200reps5)#Replace sq in 'tbhaz.200repsq' with any other decrease values as required

#' Load the census data 
census <- read.csv(paste0(path.in, "ARI census data 27-05-2024.csv"), header = T)
colnames(census) <- c("ISO3","AGEP","YARP","NUMP","CNSY","YOBP")
census <- as.data.table(census)
View(census)

#' Load TB data 
tb <- read.csv(paste0(path.in, "NNDSS skeleton.csv"), header = T)

#' DATA PREP ========================================================================================================#

#' Creating cumulative FOIs and adjusting for census year.
#' Also expanding the table's year range ( 1889 to 2016).
tbhaz.200rep <- tbhazprep.function(tbhaz.200repsq) #Replace sq in 'tbhaz.200repsq' with any other decrease values as required
View(tbhaz.200rep)

# tbhaz.5000rep <- tbhazprep.function(tbhaz.5000rep)

#' Clean the census data so that it has the following columns
#' AGEP, YARP, cob, NUMP, CNSY, YOBP and ISO3
# census <- CleanseCensus(census)

#' Clean the TB data so that it has the following columns
#' AGEP, YARP, cob, NUMP, year, YOBP and ISO3
tb <- CleanseTBdata(tb)

tb <- subset(tb, year == census.year)
tb <- as.data.table(tb)
tb[, CNSY := as.numeric(year)]

#' Checking if there are any countries that don't have an 
#' ISO3 match from Houben and Dodd.
setdiff(census$ISO3, tbhaz.200rep$iso3) 
View(census)
setdiff(tb$ISO3, census$ISO3) 

setdiff(tb$ISO3, tbhaz.200rep$iso3) 

setdiff(census$ISO3, tbhaz.200rep$iso3) #' All countries in the census data are captured in Houben & Dodd data.

#' Create a master look-up table of all probabilities of infection 
#' for each population group, by using the census and tb tables 
#' to get a list of unique ISO codes.
master.Prob <- CreateProbTables()

#' ==================edited out the ISO3 look-up for 5000 vs 200 rep
#' Subset the look-up table by ISO3 depending on whether the relevant tbhaz values 
#' are in the tbhaz.200 or tbhaz.5000 data set.
# prob.Inf5000 <- master.Prob[ISO3 == "CHN" | ISO3 == "GBR" | ISO3 == "IND" |
#                               ISO3 == "MYS" | ISO3 == "PHL" | ISO3 == "VNM"]
# 
# prob.Inf200 <- master.Prob[ ISO3 != census.iso3 & ISO3 != "CHN" & ISO3 != "GBR" 
#                             & ISO3 != "IND" & ISO3 != "MYS" & ISO3 != "PHL" 
#                             & ISO3 != "VNM"]

prob.Inf200 <- master.Prob[ !(ISO3 %in% c(census.iso3))] 
table(prob.Inf200$ISO3)
#' Also create a separate look-up table for the locally born.
#prob.Inf.local <- master.Prob[ISO3 == census.iso3]

#' Tidy
rm(master.Prob)

#' The following function calculates hazards for the locally
#' born population (prob.Inf.local). The percentiles
#' that are required can be defined earlier in the script.

#prob.Inf.local <- TBhazard.calc.function.local.born(prob.Inf.local, tbhaz.200rep)

#' Saving and removing file (memory management)
#saveRDS(prob.Inf.local, file = paste0(path.out, "prob.Inf.local.rds"))
#rm(prob.Inf.local)

#' The following function calculates hazards for the population
#' groups in the prob.Inf200 look-up table. The percentiles
#' that are required can be defined earlier in the script.
#' Because the hazard calculations are quite memory intensive, 
#' I've split the data into groups by year of birth, before 
#' running the function on it, so the chunks are more manageable.
#' Then the separate chunks are subsequently bound together again.

prob.Inf200[YOBP < 1930 , YOBPgroup := 1]
prob.Inf200[YOBP > 1929 & YOBP < 1960 , YOBPgroup := 2]
prob.Inf200[YOBP > 1959 & YOBP < 1980 , YOBPgroup := 3]
prob.Inf200[YOBP > 1979 & YOBP < 2000 , YOBPgroup := 4]
prob.Inf200[YOBP > 1999 & YOBP < 2010 , YOBPgroup := 5]
prob.Inf200[YOBP > 2009 , YOBPgroup := 6]

prob.Inf200 <- subset(prob.Inf200, CNSY >= YARP)
yob.split <- split(prob.Inf200, prob.Inf200$YOBPgroup)

yob.split <- lapply(yob.split, TBhazard.calc.function.overseas.born, tbhaz.200rep)
View(yob.split[[1]])

prob.Inf200 <- do.call("rbind", yob.split)


View(prob.Inf200)
#View(prob.Inf200)

#' Tidy
#'
rm(yob.split)

#' Saving and removing file (memory management)
saveRDS(prob.Inf200, file = paste0(path.out, "prob.Inf200sq.rds")) #Replace sq in 'prob.Inf200sq.rds' with any other decrease values as required

#' Tidy
#rm(prob.Inf200)

#' Repeat the same as above for the 5000rep look-up table.
#' However, with this table it is simplest to split it by ISO3, 
#' rather than year of birth (because there are only six ISO3).

#=================Removing iso3.split=============================
#iso3.split <- split(prob.Inf5000, prob.Inf5000$ISO3)
#' 
#iso3.split <- lapply(iso3.split, TBhazard.calc.function.overseas.born, tbhaz.5000rep)
#' 
#prob.Inf5000 <- do.call("rbind", iso3.split)
#' 
#' #' Saving and removing file (memory management)
#saveRDS(prob.Inf5000, file = paste0(path.out, "prob.Inf5000.rds"))
#rm(prob.Inf5000, iso3.split)

#' Order the census data.table.
census <- setorder(census, ISO3, YOBP, YARP)

#' Load the look-up tables back in.
#prob.Inf200 <- readRDS(paste0(path.out, "prob.Inf200sq.rds")) #Replace sq in 'prob.Inf200sq.rds' with any other decrease values as required
#prob.Inf5000 <- readRDS(paste0(path.out, "prob.Inf5000.rds"))
#prob.Inf.local <- readRDS(paste0(path.out, "prob.Inf.local.rds"))

#' Bind the look-up tables altogether to make the master look-up.
prob.Inf <- prob.Inf200

#' Tidy ==========EDITED OUT prob.Inf5000
#rm(prob.Inf.local, prob.Inf200)

#' Calculating the probability of infection (PROB)
#' from the hazards.
#' 
rm(tbhaz.200rep)


prob.Inf[, negPROB := .(mapply('*',-1, H, SIMPLIFY = F)),]
prob.Inf[, H := NULL,]
prob.Inf[, prePROB := .(mapply(exp, negPROB, SIMPLIFY = F)),]
prob.Inf[, negPROB := NULL,]
prob.Inf[, PROB := .(mapply('-', 1, prePROB, SIMPLIFY = F)),]
prob.Inf[, prePROB := NULL,]

#View(prob.Inf)
# prob.Inf[, PROB.med := 1 - exp(-(H.med))]
# prob.Inf[, PROB.low := 1 - exp(-(H.low))]
# prob.Inf[, PROB.high := 1 - exp(-(H.high))]

census <- as.data.table(census)
census[prob.Inf, LTBP := .(mapply('*', NUMP, PROB, SIMPLIFY = F)), on = .(ISO3, YOBP, YARP, CNSY),]
#View(census)
# census[prob.Inf, LTBP := NUMP * PROB.med, on = .(ISO3, YOBP, YARP)]
# census[prob.Inf, LTBP.low := NUMP * PROB.low, on = .(ISO3, YOBP, YARP)]
# census[prob.Inf, LTBP.high := NUMP * PROB.high, on = .(ISO3, YOBP, YARP)]

census_na <- na.omit(census)
#View(prob.Inf)
length(unique(census_na$ISO3))

#' Check the number missing LTBP information
census[is.na(LTBP), sum(NUMP)]

#' Check the percentage missing LTBP information
#' (some investigation may need to be done to work out
#' why these population groups have missing data. It
#' could be because some countries of birth were not
#' mapped to an ISO3 value).
census[is.na(LTBP), sum(NUMP)]/census[, sum(NUMP)] * 100

prob.Inf <- subset(prob.Inf, YARP == CNSY)

#' Merge in the TB data
#census <- merge(census, tb, by = c("AGEP", "ISO3", "CNSY", "YARP"), all.x = T)
#View(census)

#write.csv(census,"/Users/ajordan/OneDrive - McGill University/LTBI-Aust-CEA-master/Data/Outputs/2021 estimates_iso3.csv", row.names = FALSE)

#' DATA OUTPUTS =======================================================================================================#

#' Save all the objects that haven't yet been saved.
saveRDS(prob.Inf, file = paste0(path.out, "2021.prob.Inf.iso3sq.rds")) #Replace sq in '2021.prob.Inf.iso3sq.rds' with any other decrease values as required
saveRDS(census, file = paste0(path.out, "2021.estimates.iso3sq.rds"))  #Replace sq in '2021.estimates.iso3sq.rds' with any other decrease values as required

# May have removed YOBP > YARP too early

# Only consider values for last 2 years=============================================================================================================================================================
censusP <- readRDS(paste0(path.out, "2021.estimates.iso3sq.rds")) #Replace sq in '2021.estimates.iso3sq.rds' with any other decrease values as required
FCensusP <- censusP[censusP$YARP - censusP$YOBP == 2, ]
FCensusP$LTBP <- lapply(FCensusP$LTBP, function(x) as.list(x))
ECensus <- FCensusP %>% unnest(LTBP)
head(ECensus)

ECensusF <- ECensus %>%
  group_by(ISO3, AGEP, YARP, NUMP, CNSY, YOBP) %>% 
  mutate(replicate = row_number()) %>%
  ungroup()
head(ECensusF)

saveRDS(ECensusF, file = paste0(path.out, "censusLTBP(last2yr)sq.rds")) #Replace sq in 'censusLTBP(last2yr)sq.rds' with any other decrease values as required

#=======================================================================================
two_sq <- readRDS(paste0(path.out, "censusLTBP(last2yr)sq.rds"))
two_sq <- two_sq %>%
  rename(LTBPsq = LTBP)

two_1 <- readRDS(paste0(path.out, "censusLTBP(last2yr)s1.rds"))
two_1 <- two_1 %>%
  rename(LTBP1 = LTBP)

two_2 <- readRDS(paste0(path.out, "censusLTBP(last2yr)s2.rds"))
two_2 <- two_2 %>%
  rename(LTBP2 = LTBP)

two_3 <- readRDS(paste0(path.out, "censusLTBP(last2yr)s3.rds"))
two_3 <- two_3 %>%
  rename(LTBP3 = LTBP)

two_4 <- readRDS(paste0(path.out, "censusLTBP(last2yr)s4.rds"))
two_4 <- two_4 %>%
  rename(LTBP4 = LTBP)

two_5 <- readRDS(paste0(path.out, "censusLTBP(last2yr)s5.rds"))
two_5 <- two_5 %>%
  rename(LTBP5 = LTBP)

df_combined <- two_sq %>%
  mutate(
    LTBP1 = two_1$LTBP1,
    LTBP2 = two_2$LTBP2,
    LTBP3 = two_3$LTBP3,
    LTBP4 = two_4$LTBP4,
    LTBP5 = two_5$LTBP5
  )

df_combined
df_reordered <- df_combined %>%
  select(ISO3, AGEP, YARP, NUMP, CNSY, YOBP, replicate, LTBPsq, LTBP1, LTBP2, LTBP3, LTBP4, LTBP5)
df_reordered

df_reordered <- df_reordered %>%
  mutate(
    LTBPsq = as.numeric(LTBPsq),
    LTBP1 = as.numeric(LTBP1),
    LTBP2 = as.numeric(LTBP2),
    LTBP3 = as.numeric(LTBP3),
    LTBP4 = as.numeric(LTBP4),
    LTBP5 = as.numeric(LTBP5)
  )

census_last2yr <- df_reordered %>%
  mutate(
    LTBPsq_LTBP1 = LTBPsq - LTBP1,
    LTBPsq_LTBP2 = LTBPsq - LTBP2,
    LTBPsq_LTBP3 = LTBPsq - LTBP3,
    LTBPsq_LTBP4 = LTBPsq - LTBP4,
    LTBPsq_LTBP5 = LTBPsq - LTBP5
  )
census_last2yr

saveRDS(census_last2yr, file = paste0(path.out, "censusLTBP(last2yr)diff.rds"))

